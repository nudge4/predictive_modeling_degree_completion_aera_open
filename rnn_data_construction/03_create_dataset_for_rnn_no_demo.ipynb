{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to create the dataset for running the RNN models without including demographic predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "fpath = \"/Users/ys8mz/Box Sync/Predictive Models of College Completion (VCCS)/intermediate_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_stata(fpath + \"/full_data_truncated.dta\")\n",
    "df.loc[:,'available_sum'] = 0\n",
    "for p in [p for p in list(df.columns)[10:] if p.startswith(\"available\") and p != \"available_sum\"]:\n",
    "    df.loc[:,'available_sum'] += df[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = df[df.valid == 0].iloc[:,[0,1,7]+list(range(10,df.shape[1]))]\n",
    "test_part = df[df.valid == 1].iloc[:,[0,1,7]+list(range(10,df.shape[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impute_list_1 = set([\"prop_comp_pre\",\"cum_gpa_pre\"])\n",
    "impute_list_2 = set([\"cum_gpa\", \"lvl2_prop_comp\", \"dev_prop_comp\", \"prop_comp\", \"prop_comp_sd\", \"withdrawn_prop_comp_sd\"])\n",
    "impute_list_3 = set([\"admrate\", \"gradrate\", \"satvr25\", \"satvr75\", \"satmt25\", \"satmt75\", \"satwr25\", \"satwr75\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def impute(train, test):\n",
    "    for p in impute_list_1:\n",
    "        avg_p = np.nanmean(train[train.enrolled_pre == 1][p])\n",
    "        train.loc[:,p] = train.loc[:,p].apply(lambda x: avg_p if pd.isnull(x) else x)\n",
    "        test.loc[:,p] = test.loc[:,p].apply(lambda x: avg_p if pd.isnull(x) else x)\n",
    "    for p in impute_list_2:\n",
    "        avg_p = np.nanmean(train[p])\n",
    "        train.loc[:,p] = train.loc[:,p].apply(lambda x: avg_p if pd.isnull(x) else x)\n",
    "        test.loc[:,p] = test.loc[:,p].apply(lambda x: avg_p if pd.isnull(x) else x)\n",
    "    for p in impute_list_3:\n",
    "        avg_p = np.nanmean(train[train[\"enrolled_nsc\"] == 1][p])\n",
    "        train.loc[:,p] = train.loc[:,p].apply(lambda x: avg_p if pd.isnull(x) else x)\n",
    "        test.loc[:,p] = test.loc[:,p].apply(lambda x: avg_p if pd.isnull(x) else x)\n",
    "    return train, test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new, test_part_new = impute(train, test_part)\n",
    "train_part, valid_part = train_test_split(train_new, test_size=0.1, \n",
    "                                          stratify=train['grad_6years'].astype(str)+\"_\"+train['available_sum'].astype(str),\n",
    "                                          random_state=54321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors_2 = [p for p in df.columns.values if p[-4:] not in [\"_\"+v1+str(v2) for v1 in ['fa','sp','su','yr'] for v2 in range(1,7,1)]][10:]\n",
    "predictors_2 = [p for p in predictors_2 if p != \"available_sum\"]\n",
    "predictors_2 = [p for p in predictors_2 if p not in [\"age_entry\", \"male\", \"white\", \"afam\", \"hisp\", \"other\", \"pell_0_ind\", \"pell_1_ind\"] + [\"phe_\" + str(i) for i in range(1,8)]]\n",
    "len(predictors_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part2: Non-term-specific predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part2_train = train_part.loc[:, ['vccsid']+predictors_2+['grad_6years']].sort_values(['vccsid'])\n",
    "part2_valid = valid_part.loc[:, ['vccsid']+predictors_2+['grad_6years']].sort_values(['vccsid'])\n",
    "part2_test = test_part_new.loc[:, ['vccsid']+predictors_2+['grad_6years']].sort_values(['vccsid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part2_train_X = part2_train.loc[:,predictors_2].values\n",
    "part2_valid_X = part2_valid.loc[:,predictors_2].values\n",
    "part2_test_X = part2_test.loc[:,predictors_2].values\n",
    "train_y = np.array(part2_train.grad_6years)\n",
    "valid_y = np.array(part2_valid.grad_6years)\n",
    "test_y = np.array(part2_test.grad_6years)\n",
    "\n",
    "scaler_2 = MinMaxScaler(feature_range=(-1,1))\n",
    "part2_train_X = scaler_2.fit_transform(part2_train_X)\n",
    "part2_valid_X = scaler_2.transform(part2_valid_X)\n",
    "part2_test_X = scaler_2.transform(part2_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(fpath + \"/lstm_data_2/part2_train_X\", part2_train_X)\n",
    "np.save(fpath + \"/lstm_data_2/part2_valid_X\", part2_valid_X)\n",
    "np.save(fpath + \"/lstm_data_2/part2_test_X\", part2_test_X)\n",
    "np.save(fpath + \"/lstm_data_2/train_y\", train_y)\n",
    "np.save(fpath + \"/lstm_data_2/valid_y\", valid_y)\n",
    "np.save(fpath + \"/lstm_data_2/test_y\", test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part1: Term-specific predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part1 = pd.read_stata(\"C:\\\\Users\\\\ys8mz\\\\Box Sync\\\\Predictive Models of College Completion (VCCS)\\\\intermediate_files\\\\term_specific_part.dta\")\n",
    "part1_train = part2_train.loc[:,['vccsid']].merge(part1, how='inner', on=['vccsid']).sort_values(['vccsid','strm'])\n",
    "part1_train = part1_train.drop(['pell_0_', 'pell_1_'], axis=1)\n",
    "assert len(np.unique(part1_train.vccsid)) == part2_train.shape[0]\n",
    "part1_valid = part2_valid.loc[:,['vccsid']].merge(part1, how='inner', on=['vccsid']).sort_values(['vccsid','strm'])\n",
    "part1_valid = part1_valid.drop(['pell_0_', 'pell_1_'], axis=1)\n",
    "assert len(np.unique(part1_valid.vccsid)) == part2_valid.shape[0]\n",
    "part1_test = part2_test.loc[:,['vccsid']].merge(part1, how='inner', on=['vccsid']).sort_values(['vccsid','strm'])\n",
    "part1_test = part1_test.drop(['pell_0_', 'pell_1_'], axis=1)\n",
    "assert len(np.unique(part1_test.vccsid)) == part2_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_id_rng(dat):\n",
    "    l = []\n",
    "    crnt_id = None\n",
    "    for i in range(dat.shape[0]):\n",
    "        vccsid = dat.vccsid.iloc[i]\n",
    "        if vccsid != crnt_id:\n",
    "            if crnt_id is not None:\n",
    "                l.append((start_indx, i))\n",
    "            start_indx = i\n",
    "            crnt_id = vccsid\n",
    "    l.append((start_indx, dat.shape[0]))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_id_rng = create_id_rng(part1_train)\n",
    "valid_id_rng = create_id_rng(part1_valid)\n",
    "test_id_rng = create_id_rng(part1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler_1 = MinMaxScaler(feature_range=(-1,1))\n",
    "part1_train_X = scaler_1.fit_transform(part1_train.iloc[:,4:].values)\n",
    "part1_valid_X = scaler_1.transform(part1_valid.iloc[:,4:].values)\n",
    "part1_test_X = scaler_1.transform(part1_test.iloc[:,4:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part1_train_X = [part1_train_X[i1:i2,:] for i1,i2 in train_id_rng]\n",
    "part1_valid_X = [part1_valid_X[i1:i2,:] for i1,i2 in valid_id_rng]\n",
    "part1_test_X = [part1_test_X[i1:i2,:] for i1,i2 in test_id_rng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(part1_train_X) == part2_train_X.shape[0]\n",
    "assert len(part1_valid_X) == part2_valid_X.shape[0]\n",
    "assert len(part1_test_X) == part2_test_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for v in ['train','valid','test']:\n",
    "    v_name = \"part1_{}_X\".format(v)\n",
    "    pickle.dump(eval(v_name), open(fpath+\"/lstm_data_2/\"+v_name+\".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
