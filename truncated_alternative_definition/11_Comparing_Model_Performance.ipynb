{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script attempts to provide more \"apple-to-apple\" comparison for the performance between the two outcome definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import gmean\n",
    "import sklearn\n",
    "import statsmodels.formula.api as sm\n",
    "from statsmodels.tools import add_constant\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from statsmodels.genmod import families\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "fpath = \"/Users/ys8mz/Box Sync/Predictive Models of College Completion (VCCS)/intermediate_files\"\n",
    "fpath_1 = \"/Users/ys8mz/Box Sync/Predictive Models of College Completion (VCCS)/evaluation_results/truncated_predictors/cleaned_results/\"\n",
    "fpath_2 = \"/Users/ys8mz/Box Sync/Predictive Models of College Completion (VCCS)/evaluation_results/truncated_new_5/cleaned_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_old = pd.read_stata(fpath + \"/full_data_truncated.dta\").loc[:,['vccsid','valid','grad_6years','first_degree_strm']]\n",
    "df_old = df_old[df_old.valid == 1].drop(['valid'], axis=1)\n",
    "df_new = pd.read_stata(fpath + \"/full_data_truncated_alternative.dta\").loc[:,['vccsid','valid','grad_6years','first_degree_strm']]\n",
    "df_new = df_new[df_new.valid == 1].drop(['valid'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focus on the performance comparison of observations where the outcome is the same according to both definitions (either the student earned the 1st degree at VCCS, or the student never earned any degree during the 6-year window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29679, 5)\n",
      "(29636, 5)\n"
     ]
    }
   ],
   "source": [
    "y = df_old.merge(df_new, how='left', on=['vccsid'])\n",
    "y = y[y.grad_6years_x == y.grad_6years_y]\n",
    "print(y.shape)\n",
    "y = y[np.array(y.first_degree_strm_x == y.first_degree_strm_y) | np.array(pd.isnull(y.first_degree_strm_x))]\n",
    "print(y.shape)\n",
    "y = y.iloc[:,:2]\n",
    "y.columns = ['vccsid', 'y_real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26413822174072266"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Share of students who earned the 1st degree at VCCS in the validation set\n",
    "np.mean(y.y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_1 = pd.read_csv(fpath_1 + \"all_pred_scores.csv\").rename(columns={\"Unnamed: 0\": \"vccsid\"}).merge(y, on=['vccsid'],how='inner')\n",
    "score_2 = pd.read_csv(fpath_2 + \"all_pred_scores.csv\").rename(columns={\"Unnamed: 0\": \"vccsid\"}).merge(y, on=['vccsid'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('OLS', 0.9112265183703853, 0.9132358817567428)\n",
      "('Logit', 0.9130524950457206, 0.9228889765317495)\n",
      "('RF', 0.9202382385306466, 0.9262694274573918)\n",
      "('XGBoost', 0.9334251263688, 0.9437862631968842)\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance (c-statistics) of the alternative definition model with the original definition on the focus group\n",
    "r_list = []\n",
    "for m in ['OLS','Logit','RF','XGBoost']:\n",
    "    r_list.append((m, roc_auc_score(score_1.y_real, score_1[m]),roc_auc_score(score_2.y_real, score_2[m])))\n",
    "    print(r_list[-1])\n",
    "pd.DataFrame(r_list, columns=['model','base_cstat','no_nsc_cstat']).round(4).to_csv(fpath_2 + \"/new_cstat_comparison.csv\", index=False)\n",
    "# The alternative definiton model slightly outperform the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the mean balanced F1 score for non-graduation for the two outcome definitions through random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_old_score = df_old.merge(pd.read_csv(fpath_1 + \"all_pred_scores.csv\").rename(columns={\"Unnamed: 0\": \"vccsid\"}), on=['vccsid'], how='inner')\n",
    "df_new_score = df_new.merge(pd.read_csv(fpath_2 + \"all_pred_scores.csv\").rename(columns={\"Unnamed: 0\": \"vccsid\"}), on=['vccsid'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_old_score_1 = df_old_score[df_old_score.grad_6years == 1]\n",
    "df_old_score_0 = df_old_score[df_old_score.grad_6years == 0]\n",
    "df_new_score_1 = df_new_score[df_new_score.grad_6years == 1]\n",
    "df_new_score_0 = df_new_score[df_new_score.grad_6years == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_df = pd.read_csv(fpath_1 + \"/main_eval_metrics.csv\").iloc[:,[0,2]].merge(pd.read_csv(fpath_2 + \"/main_eval_metrics.csv\").iloc[:,[0,2]], how='inner', on=['model'])\n",
    "thresholds = {}\n",
    "for i in range(t_df.shape[0]):\n",
    "    thresholds[t_df.iloc[i,0]] = tuple(t_df.iloc[i,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n"
     ]
    }
   ],
   "source": [
    "random.seed(4321)\n",
    "n=5000\n",
    "f1_list = []\n",
    "for i in range(n):\n",
    "    if i % 200 == 0:\n",
    "        print(i)\n",
    "    fl = []\n",
    "    df_old_score_sample = pd.concat([df_old_score_1.sample(5000, replace = True),df_old_score_0.sample(5000, replace = True)])\n",
    "    df_new_score_sample = pd.concat([df_new_score_1.sample(5000, replace = True),df_new_score_0.sample(5000, replace = True)])\n",
    "    for m in ['OLS','Logit','RF','XGBoost']:\n",
    "        fl.append(f1_score(df_old_score_sample.grad_6years, np.where(df_old_score_sample[m]>thresholds[m][0],1,0), pos_label=0))\n",
    "        fl.append(f1_score(df_new_score_sample.grad_6years, np.where(df_new_score_sample[m]>thresholds[m][0],1,0), pos_label=0))\n",
    "    f1_list.append(tuple(fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'model':['OLS']*2 + ['Logit']*2 + ['RF']*2 + ['XGBoost']*2,\n",
    "              'variant':['base','no_nsc']*4,\n",
    "              'mean_f1_score_0': np.array(f1_list).mean(axis=0),'std':np.array(f1_list).std(axis=0)}).loc[:,['model','variant','mean_f1_score_0','std']].round(4).to_csv(fpath_2 + \"/adjusted_f1_score_0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A sample comparison for XGBoost models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8345588235294118"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_old_score_sample = pd.concat([df_old_score_1.sample(5000, replace = True),df_old_score_0.sample(5000, replace = True)])\n",
    "f1_score(df_old_score_sample.grad_6years, np.where(df_old_score_sample['XGBoost']>0.3853,1,0), pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8558918713116315"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_score_sample = pd.concat([df_new_score_1.sample(5000, replace = True),df_new_score_0.sample(5000, replace = True)])\n",
    "f1_score(df_new_score_sample.grad_6years, np.where(df_new_score_sample['XGBoost']>0.3849,1,0), pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
